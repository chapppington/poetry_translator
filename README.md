# Poetry Translator

Переводчик стихотворений с использованием стиля конкретного переводчика на основе RAG (Retrieval-Augmented Generation) и Ollama.

## Установка

### 1. Установка Ollama

Для работы проекта необходим Ollama, установленный нативно (не в Docker):

```bash
# Установка через Homebrew
brew install ollama

# Запуск Ollama
ollama serve
```

**Важно:** Используйте нативный Ollama для доступа к GPU (Metal) на Apple Silicon. Это значительно ускорит работу.

### 2. Загрузка моделей

В другом терминале загрузите необходимые модели:

```bash
# Модель для генерации переводов
ollama pull gemma3:12b

# Модель для создания embeddings (для RAG)
ollama pull nomic-embed-text
```

### 3. Установка зависимостей Python

Проект использует Poetry для управления зависимостями:

```bash
# Установка Poetry (если еще не установлен)
curl -sSL https://install.python-poetry.org | python3 -

# Установка зависимостей проекта
poetry install
```

Или используйте pip напрямую:

```bash
pip install requests numpy
```

## Подготовка данных

### Индексация стихотворений

Перед использованием переводчика необходимо проиндексировать стихотворения из папки `poems/`:

```bash
# Индексировать все JSON файлы из папки poems
python rag_index.py --dir ./poems

# Индексировать один файл
python rag_index.py poems/poem_gotham.json

# Указать имя индекса
python rag_index.py --dir ./poems --name poems_index
```

**Структура проекта:**
- `poems/` - папка с JSON файлами стихотворений
- `rag_index/` - папка с сохраненными индексами (создается автоматически)

**Формат JSON файлов:**
JSON файлы должны содержать поля:
- `title_original` - название оригинала
- `title_translation` - название перевода (опционально)
- `author` - автор оригинала
- `translator` - переводчик
- `original` - текст оригинала
- `translation` - текст перевода
- `comment` - комментарий о стиле перевода (опционально)

## Использование

### Перевод стихотворений

Основной скрипт `simple_translator.py` переводит стихотворения в стиле указанного переводчика:

```bash
# Перевести стихотворение из файла в стиле Маршака
python simple_translator.py --file poem.txt --style "Маршак"

# Перевести текст напрямую
python simple_translator.py "Humpty-Dumpty sat on a wall" --style "Маршак"
```

**Особенности перевода:**
- Переводчик создает полностью рифмованный перевод
- Каждая строка рифмуется с парной строкой (схема AABB или ABAB)
- Использует примеры переводов указанного переводчика из индекса для понимания стиля
- Сохраняет смысл оригинала, но может творчески менять слова ради рифмы и музыкальности

### Как это работает

1. **Поиск примеров**: Скрипт ищет в индексе примеры переводов указанного переводчика (например, "Маршак")
2. **Системный промпт**: Создается промпт с инструкциями по переводу в нужном стиле
3. **Контекст**: Найденные примеры используются как контекст для понимания стиля
4. **Перевод**: Модель переводит ваше стихотворение, учитывая стиль переводчика

**Важно**: Для работы нужно проиндексировать JSON файлы с переводами нужного переводчика. 

Если в JSON файлах указан `"translator": "Самуил Маршак"`, то при запросе `--style "Маршак"` скрипт найдет эти примеры и использует их для понимания стиля.

## RAG (Retrieval-Augmented Generation)

RAG позволяет использовать проиндексированные переводы стихотворений для генерации переводов с учетом стиля конкретного переводчика.

### Как работает индексация

1. **Разбиение на чанки**: Документы разбиваются на небольшие фрагменты (чанки)
2. **Создание embeddings**: Для каждого чанка создается векторное представление (embedding)
3. **Сохранение индекса**: Индекс сохраняется в папке `rag_index/` в формате pickle

### Поиск по индексу

При запросе перевода:
1. Скрипт ищет в индексе все чанки, содержащие имя переводчика
2. Найденные примеры переводов используются как контекст
3. Модель генерирует перевод, учитывая стиль из примеров

## Требования

- Python 3.13+
- Ollama (нативная установка, не Docker)
- Модели Ollama: `gemma3:12b` и `nomic-embed-text`
- Зависимости: `requests`, `numpy`

## Примечания

- Модель будет сохранена в `~/.ollama`
- Конфигурация оптимизирована для macOS на ARM архитектуре (Apple Silicon)
- Нативный Ollama использует Metal GPU для ускорения (в 3-10 раз быстрее, чем CPU)
- Для работы с большими моделями убедитесь, что у вас достаточно RAM
